# FireFighter drone RL

**English**: A drone coordination system for firefighting using Reinforcement Learning (RL) methods.  
**–†—É—Å—Å–∫–∏–π**: –°–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –¥—Ä–æ–Ω–æ–≤ –¥–ª—è —Ç—É—à–µ–Ω–∏—è –ø–æ–∂–∞—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning).

## üìã Table of Contents / –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [Project Overview / –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞](#project-overview--–æ–±–∑–æ—Ä-–ø—Ä–æ–µ–∫—Ç–∞)
- [Key Features / –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏](#key-features--–∫–ª—é—á–µ–≤—ã–µ-–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏)
- [Project Structure / –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞](#project-structure--—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–ø—Ä–æ–µ–∫—Ç–∞)
- [Simulation Environment / –°—Ä–µ–¥–∞ —Å–∏–º—É–ª—è—Ü–∏–∏](#simulation-environment--—Å—Ä–µ–¥–∞-—Å–∏–º—É–ª—è—Ü–∏–∏)
- [Learning Algorithm / –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è](#learning-algorithm--–∞–ª–≥–æ—Ä–∏—Ç–º-–æ–±—É—á–µ–Ω–∏—è)
- [Visualization and Interface / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å](#visualization-and-interface--–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è-–∏-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
- [Running the Project / –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞](#running-the-project--–∑–∞–ø—É—Å–∫-–ø—Ä–æ–µ–∫—Ç–∞)
- [Use Cases / –°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](#use-cases--—Å—Ü–µ–Ω–∞—Ä–∏–∏-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
- [Performance Metrics / –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏](#performance-metrics--–º–µ—Ç—Ä–∏–∫–∏-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
- [MongoDB Integration / –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MongoDB](#mongodb-integration--–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-mongodb)
- [Result Interpretation / –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](#result-interpretation--–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)

## Project Overview / –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞

**English**: FireFighterRL is a simulation environment and trainable model for managing a team of drones in firefighting tasks. It leverages reinforcement learning to optimize firefighting strategies across various scenarios, accounting for obstacles, environmental conditions (e.g., wind), and resource constraints (e.g., battery life).  

**–†—É—Å—Å–∫–∏–π**: FireFighterRL ‚Äî —ç—Ç–æ —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –∏ –æ–±—É—á–∞–µ–º–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥–æ–π –¥—Ä–æ–Ω–æ–≤ –ø—Ä–∏ —Ç—É—à–µ–Ω–∏–∏ –ø–æ–∂–∞—Ä–æ–≤. –ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∂–∞—Ä–æ—Ç—É—à–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —É—á–∏—Ç—ã–≤–∞—è –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è, –ø—Ä–∏—Ä–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–µ—Ç–µ—Ä) –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞—Ä—è–¥ –±–∞—Ç–∞—Ä–µ–∏).

## Key Features / –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

**English**:
- ü§ñ Multi-agent control of a team of 3 drones
- üîÑ Two distinct simulation scenarios (FireEnv and FireEnv2)
- üå™Ô∏è Modeling of external factors (wind, obstacles)
- üéÆ Visualization using Pygame
- üß† Model training with Proximal Policy Optimization (PPO)
- üîß Hyperparameter optimization using Optuna
- üìä Testing and evaluation of trained models

**–†—É—Å—Å–∫–∏–π**:
- ü§ñ –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–æ–π –∏–∑ 3 –¥—Ä–æ–Ω–æ–≤
- üîÑ –î–≤–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ (FireEnv –∏ FireEnv2)
- üå™Ô∏è –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (–≤–µ—Ç–µ—Ä, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è)
- üéÆ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Pygame
- üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PPO
- üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna
- üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

## Project Structure / –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
FireFighterRL/
‚îú‚îÄ‚îÄ app.py                  # Main application / –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
‚îú‚îÄ‚îÄ constants/              # Constants / –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ colors.py           # Colors for visualization / –¶–≤–µ—Ç–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
‚îÇ   ‚îú‚îÄ‚îÄ agent.py            # Agent state constants / –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
‚îÇ   ‚îî‚îÄ‚îÄ grid.py             # Grid and visualization constants / –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å–µ—Ç–∫–∏
‚îú‚îÄ‚îÄ data/                   # Application data / –î–∞–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ best_model/         # Saved best models / –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ images/             # Images for visualization / –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ envs/                   # Gymnasium environments / –û–∫—Ä—É–∂–µ–Ω–∏—è Gymnasium
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Environment constants / –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å—Ä–µ–¥—ã
‚îÇ   ‚îú‚îÄ‚îÄ FireEnv.py          # Main firefighting environment / –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ä–µ–¥–∞
‚îÇ   ‚îú‚îÄ‚îÄ FireEnv2.py         # Alternative environment / –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞
‚îÇ   ‚îú‚îÄ‚îÄ reward_sys.py       # Reward function for scenario 1 / –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ reward_sys2.py      # Reward function for scenario 2 / –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥ –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ Fire.py             # Fire spread simulation / –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ–≥–Ω—è
‚îÇ   ‚îî‚îÄ‚îÄ Wind.py             # Wind simulation / –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ç—Ä–∞
‚îú‚îÄ‚îÄ logs/                   # Logs and results / –õ–æ–≥–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ logs.csv            # Testing logs / –õ–æ–≥–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ test_logs.csv       # Detailed test logs / –ü–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ —Ç–µ—Å—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ test_rewards.csv    # Detailed reward logs / –ü–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ –Ω–∞–≥—Ä–∞–¥
‚îÇ   ‚îî‚îÄ‚îÄ ppo_tensorboard/    # TensorBoard logs / –õ–æ–≥–∏ –¥–ª—è TensorBoard
‚îú‚îÄ‚îÄ main.py                 # Entry point / –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
‚îú‚îÄ‚îÄ models/                 # RL models / –ú–æ–¥–µ–ª–∏ RL
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_config.py     # PPO model configuration / –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PPO
‚îÇ   ‚îú‚îÄ‚îÄ optuna_train.py     # Hyperparameter optimization / –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ test_model.py       # Model testing / –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ TrainingCallBack.py # Training callbacks / –ö–æ–ª–ª–±—ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ train_model.py      # Model training / –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ mongo/                  # Visualization / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ mongo_integration_for_training.py  # MongoDB integration for training / –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è MongoDB –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ mongo_integration.py               # MongoDB connection / –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å MongoDB
‚îÇ   ‚îî‚îÄ‚îÄ mongo_test_integration.py          # MongoDB integration for testing / –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è MongoDB –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
‚îú‚îÄ‚îÄ render/                 # Visualization / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ load_images.py      # Image loading / –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
‚îÇ   ‚îî‚îÄ‚îÄ user_interface.py   # User interface / –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ utils/                  # Utilities / –£—Ç–∏–ª–∏—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ logger.py           # Logging / –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ get_console_data.py # Console logging / –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ logging_files.py    # Log file paths / –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –ª–æ–≥–æ–≤
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt        # Dependencies / –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```

## Simulation Environment / –°—Ä–µ–¥–∞ —Å–∏–º—É–ª—è—Ü–∏–∏

**English**: The environment is built using the Gymnasium framework with the following characteristics:  
- üèÅ 20√ó20 grid  
- üöÅ Team of 3 drones starting from a base  
- üî• Randomly distributed fire sources  
- üöß Obstacles to navigate  
- üí® Random wind affecting drone movement  
- üîã Step limit based on drone battery  

**–†—É—Å—Å–∫–∏–π**: –°—Ä–µ–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ Gymnasium –∏ –∏–º–µ–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:  
- üèÅ –°–µ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–º 20√ó20 –∫–ª–µ—Ç–æ–∫  
- üöÅ –ö–æ–º–∞–Ω–¥–∞ –∏–∑ 3 –¥—Ä–æ–Ω–æ–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Å –±–∞–∑—ã  
- üî• –°–ª—É—á–∞–π–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –æ—á–∞–≥–∏ –ø–æ–∂–∞—Ä–æ–≤  
- üöß –ü—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ö–æ–¥–∏—Ç—å  
- üí® –°–ª—É—á–∞–π–Ω—ã–π –≤–µ—Ç–µ—Ä, –≤–ª–∏—è—é—â–∏–π –Ω–∞ –¥–≤–∏–∂–µ–Ω–∏–µ –¥—Ä–æ–Ω–æ–≤  
- üîã –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä –¥—Ä–æ–Ω–∞)  

### Reward System / –°–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥

| **Action / –î–µ–π—Å—Ç–≤–∏–µ**                     | **Reward / –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ**                                      |
|-------------------------------------------|------------------------------------------------------------------|
| Extinguishing a fire / –¢—É—à–µ–Ω–∏–µ –ø–æ–∂–∞—Ä–∞     | +1.0                                                             |
| Quickly extinguishing all fires / –ë—ã—Å—Ç—Ä–æ–µ —Ç—É—à–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–∂–∞—Ä–æ–≤ | +5.0 (increases with faster completion) / +5.0 (—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏) |
| Approaching a fire / –ü—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –∫ –æ–≥–Ω—é   | +0.05                                                            |
| Colliding with an obstacle / –°—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–µ —Å –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ–º | -0.2                                                             |
| Colliding with another drone / –°—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º –¥—Ä–æ–Ω–æ–º | -0.3                                                             |
| Wind impact / –í–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –≤–µ—Ç—Ä–∞           | -0.15                                                            |
| Idling (no action) / –ó–∞—Å—Ç–∞–∏–≤–∞–Ω–∏–µ         | -0.1                                                             |
| Step / –®–∞–≥                                | -0.02                                                            |

## Learning Algorithm / –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è

**English**: The project uses the Proximal Policy Optimization (PPO) algorithm from Stable-Baselines3. Key parameters:  

```python
PPO_DEFAULT_CONFIG = {
    "policy": "MlpPolicy",        # Multi-layer perceptron policy
    "verbose": 1,                 # Logging verbosity
    "learning_rate": 0.0001,      # Learning rate
    "n_steps": 4096,              # Steps before update
    "batch_size": 256,            # Batch size
    "n_epochs": 5,                # Training epochs
    "gamma": 0.99,                # Discount factor
    "gae_lambda": 0.95,           # GAE parameter
    "clip_range": 0.2,            # Clipping range
    "clip_range_vf": 0.2,         # Value function clipping
    "ent_coef": 0.01,             # Entropy coefficient
    "total_timesteps": 100000,    # Total training steps
}
```

Hyperparameter optimization is implemented using Optuna to find the best model configuration.  

**–†—É—Å—Å–∫–∏–π**: –ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º PPO (Proximal Policy Optimization) –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Stable-Baselines3. –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:  

```python
PPO_DEFAULT_CONFIG = {
    "policy": "MlpPolicy",        # –ü–æ–ª–∏—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
    "verbose": 1,                 # –£—Ä–æ–≤–µ–Ω—å –≤—ã–≤–æ–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    "learning_rate": 0.0001,      # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    "n_steps": 4096,              # –®–∞–≥–æ–≤ –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
    "batch_size": 256,            # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    "n_epochs": 5,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    "gamma": 0.99,                # –§–∞–∫—Ç–æ—Ä –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    "gae_lambda": 0.95,           # –ü–∞—Ä–∞–º–µ—Ç—Ä GAE
    "clip_range": 0.2,            # –î–∏–∞–ø–∞–∑–æ–Ω –æ—Ç—Å–µ—á–µ–Ω–∏—è
    "clip_range_vf": 0.2,         # –î–∏–∞–ø–∞–∑–æ–Ω –æ—Ç—Å–µ—á–µ–Ω–∏—è –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
    "ent_coef": 0.01,             # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏
    "total_timesteps": 100000,    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
}
```

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Optuna –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏.

## Visualization and Interface / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

**English**: The environment and firefighting process are visualized using Pygame. The interface includes:  
- üñºÔ∏è Graphical display of the game field  
- üìä Dashboard with current state (steps, fires, reward)  
- üí¨ Dialog windows for simulation parameter setup  
- üìù Results window for test outcomes  

**–†—É—Å—Å–∫–∏–π**: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã –∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ç—É—à–µ–Ω–∏—è –ø–æ–∂–∞—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Pygame. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–∫–ª—é—á–∞–µ—Ç:  
- üñºÔ∏è –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è  
- üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–∞–Ω–µ–ª—å —Å —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (—à–∞–≥–∏, –æ—á–∞–≥–∏, –Ω–∞–≥—Ä–∞–¥–∞)  
- üí¨ –î–∏–∞–ª–æ–≥–æ–≤—ã–µ –æ–∫–Ω–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏  
- üìù –û–∫–Ω–æ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è  

## Running the Project / –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞

### Installing Dependencies / –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

### Running the Application / –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

```bash
python main.py
```

**English**: After launching, select the operation mode (training or testing) and configure environment parameters:  
- Scenario selection (1 or 2)  
- Number of fire sources  
- Number of obstacles  

**–†—É—Å—Å–∫–∏–π**: –ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã (–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ) –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã:  
- –í—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è (1 –∏–ª–∏ 2)  
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—á–∞–≥–æ–≤ –ø–æ–∂–∞—Ä–∞  
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π  

## Use Cases / –°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

**English**:
1. **Training a New Model**: Select "Training" mode, configure environment parameters, and save the trained model.  
2. **Testing a Model**: Select "Test" mode, load an existing model, and review results.  
3. **Hyperparameter Optimization**: Select "Optuna" mode, run optimization, and obtain optimal parameters.  

**–†—É—Å—Å–∫–∏–π**:
1. **–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏**: –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º "–û–±—É—á–µ–Ω–∏–µ", –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.  
2. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏**: –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º "–¢–µ—Å—Ç", –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.  
3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**: –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º "Optuna", –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ –ø–æ–ª—É—á–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.  

## Performance Metrics / –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

**English**: The following metrics evaluate model performance:  
- ‚úÖ **Success Rate**: Percentage of successfully extinguished fires  
- üìà **Average Reward**: Total reward per episode  
- ‚è±Ô∏è **Step Efficiency**: Steps required to extinguish fires  
- üí• **Collision Rate**: Average collisions per episode  

**–†—É—Å—Å–∫–∏–π**: –î–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:  
- ‚úÖ **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å**: –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ—Ç—É—à–µ–Ω–Ω—ã—Ö –ø–æ–∂–∞—Ä–æ–≤  
- üìà **–°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ**: –ò—Ç–æ–≥–æ–≤–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —ç–ø–∏–∑–æ–¥  
- ‚è±Ô∏è **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –Ω–∞ —Ç—É—à–µ–Ω–∏–µ –ø–æ–∂–∞—Ä–∞  
- üí• **–ß–∞—Å—Ç–æ—Ç–∞ —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π**: –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π –∑–∞ —ç–ø–∏–∑–æ–¥  

## MongoDB Integration / –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MongoDB

**English**: The project integrates with MongoDB for logging, storing, and analyzing experiment results:  
- üìù **Training Logging**: Real-time storage of training metrics  
- üìä **Test Results**: Structured storage of model test outcomes  
- üìà **Data Analysis Tools**: Jupyter notebooks for result analysis  
- üßÆ **Analytical Dashboards**: Visualization and experiment comparison  

Main MongoDB collections:  
- `experiments`: Training experiment details  
- `training_steps`: Metrics at training stages  
- `test_results`: Model test outcomes  
- `test_episodes`: Detailed test episode data  

**–†—É—Å—Å–∫–∏–π**: –ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å MongoDB –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è, —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:  
- üìù **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏  
- üìä **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤  
- üìà **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞**: Jupyter-–Ω–æ—É—Ç–±—É–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö  
- üßÆ **–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞—à–±–æ—Ä–¥—ã**: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤  

–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ MongoDB:  
- `experiments`: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö  
- `training_steps`: –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è  
- `test_results`: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π  
- `test_episodes`: –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ—Å—Ç–æ–≤—ã–º —ç–ø–∏–∑–æ–¥–∞–º  

### Data Analysis Capabilities / –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö

**English**: Jupyter notebooks provide advanced data analysis:  
1. **Data Structure Monitoring**: Track document counts and data integrity.  
2. **Training Visualization**: Plot reward trends and parameter impacts.  
3. **Scenario Comparison**: Compare model performance across scenarios.  
4. **Hyperparameter Analysis**: Evaluate different model configurations.  

**–†—É—Å—Å–∫–∏–π**: Jupyter-–Ω–æ—É—Ç–±—É–∫–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:  
1. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏.  
2. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –Ω–∞–≥—Ä–∞–¥ –∏ –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.  
3. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.  
4. **–ê–Ω–∞–ª–∏–∑ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**: –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π.  

## Result Interpretation / –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### Training Log Analysis / –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è

**English**: Training logs are saved in CSV format for analysis. Key parameters:  

| **Parameter / –ü–∞—Ä–∞–º–µ—Ç—Ä** | **Description / –û–ø–∏—Å–∞–Ω–∏–µ**                          |
|--------------------------|----------------------------------------------------|
| Timestep                 | Current training step / –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è       |
| Mean Reward              | Average reward per period / –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ |
| Episode Length           | Episode length (steps) / –î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞ (—à–∞–≥–∏)     |
| Fires Left               | Remaining fires / –û—Å—Ç–∞–≤—à–∏–µ—Å—è –æ—á–∞–≥–∏                |

Increasing mean reward and decreasing remaining fires indicate successful training.  

**–†—É—Å—Å–∫–∏–π**: –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:  

| **–ü–∞—Ä–∞–º–µ—Ç—Ä**             | **–û–ø–∏—Å–∞–Ω–∏–µ**                                      |
|--------------------------|--------------------------------------------------|
| Timestep                 | –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è                            |
| Mean Reward              | –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥                |
| Episode Length           | –î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤)                |
| Fires Left               | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –æ—á–∞–≥–æ–≤                    |

–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –æ—á–∞–≥–æ–≤ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —É—Å–ø–µ—à–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.

### TensorBoard Visualization / –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ TensorBoard

**English**: Use TensorBoard for in-depth analysis:  

```bash
tensorboard --logdir=./logs/ppo_tensorboard/
```

Tracks:  
- üìà Reward dynamics  
- üìâ Loss function  
- üîÑ Policy entropy  
- üí∞ Value function  

**–†—É—Å—Å–∫–∏–π**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ TensorBoard –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:  

```bash
tensorboard --logdir=./logs/ppo_tensorboard/
```

–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç:  
- üìà –î–∏–Ω–∞–º–∏–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è  
- üìâ –§—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å  
- üîÑ –≠–Ω—Ç—Ä–æ–ø–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏  
- üí∞ –§—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏  

### Test Results / –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

**English**: Example test output:  

```
Evaluation results:
Success Rate: 80.0%              # Successfully completed episodes
Average Reward: 25.45            # Average reward per episode
Step Efficiency: 120.5 steps/goal # Steps per goal
Collision Rate: 2.3 in episode    # Average collisions
```

**Interpretation**:  
- **Success Rate** > 70%: Good performance  
- **Average Reward** > 20: Effective strategy  
- **Step Efficiency** < 150: Efficient paths  
- **Collision Rate** < 3: Good obstacle avoidance  

**–†—É—Å—Å–∫–∏–π**: –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:  

```
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:
Success Rate: 80.0%              # –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
Average Reward: 25.45            # –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —ç–ø–∏–∑–æ–¥
Step Efficiency: 120.5 —à–∞–≥–æ–≤/—Ü–µ–ª—å # –®–∞–≥–æ–≤ –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏
Collision Rate: 2.3 –∑–∞ —ç–ø–∏–∑–æ–¥     # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π
```

**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**:  
- **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å** > 70%: –•–æ—Ä–æ—à–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å  
- **–°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ** > 20: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è  
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤** < 150: –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—É—Ç–∏  
- **–ß–∞—Å—Ç–æ—Ç–∞ —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π** < 3: –•–æ—Ä–æ—à–µ–µ –∏–∑–±–µ–≥–∞–Ω–∏–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π
